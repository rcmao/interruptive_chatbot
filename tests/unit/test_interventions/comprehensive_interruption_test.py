"""
ç»¼åˆæ‰“æ–­åŠŸèƒ½æµ‹è¯•æ¡†æ¶
æµ‹è¯•chatbotæ‰“æ–­çš„åˆç†æ€§ã€åŠæ—¶æ€§å’Œæœ‰æ•ˆæ€§
"""

import asyncio
import time
import json
import sys
import os
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
try:
    from src.detectors.optimized_monitor import OptimizedConflictMonitor
    from src.core.main import MessageData
    from src.interventions.intervention_generator import TKIStrategy, ConflictPhase
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿é¡¹ç›®è·¯å¾„æ­£ç¡®è®¾ç½®")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TestScenario:
    """æµ‹è¯•åœºæ™¯"""
    name: str
    description: str
    messages: List[Dict]  # æ¶ˆæ¯åºåˆ—
    expected_interventions: List[int]  # æœŸæœ›å¹²é¢„çš„æ­¥éª¤
    expected_timing: List[float]  # æœŸæœ›çš„å“åº”æ—¶é—´
    expected_strategy: str  # æœŸæœ›çš„TKIç­–ç•¥

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    scenario_name: str
    interventions_triggered: List[Dict]
    response_times: List[float]
    accuracy: float
    timing_accuracy: float
    strategy_accuracy: float
    overall_score: float

class ComprehensiveInterruptionTester:
    """ç»¼åˆæ‰“æ–­åŠŸèƒ½æµ‹è¯•å™¨"""
    
    def __init__(self, api_key: str, api_base: str):
        self.api_key = api_key
        self.api_base = api_base
        self.test_scenarios = self._create_test_scenarios()
        
    def _create_test_scenarios(self) -> List[TestScenario]:
        """åˆ›å»ºæµ‹è¯•åœºæ™¯"""
        return [
            # åœºæ™¯1: æ¸è¿›å¼å†²çªå‡çº§
            TestScenario(
                name="æ¸è¿›å¼å†²çªå‡çº§",
                description="ä»è½»å¾®åˆ†æ­§åˆ°æ¿€çƒˆå†²çªçš„å®Œæ•´è¿‡ç¨‹",
                messages=[
                    {"role": "user1", "content": "æˆ‘è§‰å¾—è¿™ä¸ªæ–¹æ¡ˆè¿˜å¯ä»¥", "expected_intervention": False},
                    {"role": "user2", "content": "æˆ‘æœ‰äº›ä¸åŒæ„è§", "expected_intervention": False},
                    {"role": "user1", "content": "ä¸ºä»€ä¹ˆï¼Ÿæœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ", "expected_intervention": False},
                    {"role": "user2", "content": "è¿™ä¸ªè®¾è®¡è€ƒè™‘ä¸å‘¨å…¨", "expected_intervention": False},
                    {"role": "user1", "content": "ä½ æ€»æ˜¯è¿™æ ·æŒ‘æ¯›ç—…", "expected_intervention": True},
                    {"role": "user2", "content": "ä½ ä»ä¸è®¤çœŸè€ƒè™‘åˆ«äººçš„æƒ³æ³•", "expected_intervention": True},
                    {"role": "user1", "content": "ä½ é”™äº†ï¼Œè¿™æ ·ç»å¯¹ä¸è¡Œï¼", "expected_intervention": True},
                ],
                expected_interventions=[5, 6, 7],  # ç¬¬5ã€6ã€7æ­¥åº”è¯¥å¹²é¢„
                expected_timing=[2.0, 1.5, 1.0],  # å“åº”æ—¶é—´åº”è¯¥è¶Šæ¥è¶Šå¿«
                expected_strategy="collaborating"
            ),
            
            # åœºæ™¯2: çªå‘æ€§æ¿€çƒˆå†²çª
            TestScenario(
                name="çªå‘æ€§æ¿€çƒˆå†²çª",
                description="çªç„¶å‡ºç°çš„æ¿€çƒˆå†²çªï¼Œéœ€è¦å¿«é€Ÿå¹²é¢„",
                messages=[
                    {"role": "user1", "content": "ä»Šå¤©å¤©æ°”ä¸é”™", "expected_intervention": False},
                    {"role": "user2", "content": "æˆ‘å—å¤Ÿäº†ä½ çš„æ— ç†å–é—¹ï¼", "expected_intervention": True},
                    {"role": "user1", "content": "ä½ æ‰æ˜¯ä»€ä¹ˆéƒ½ä¸æ‡‚ï¼", "expected_intervention": True},
                ],
                expected_interventions=[2, 3],
                expected_timing=[1.0, 0.8],  # å¿«é€Ÿå“åº”
                expected_strategy="accommodating"
            ),
            
            # åœºæ™¯3: å‘è¨€æƒäº‰å¤º
            TestScenario(
                name="å‘è¨€æƒäº‰å¤º",
                description="å¤šäººè®¨è®ºä¸­çš„å‘è¨€æƒé—®é¢˜",
                messages=[
                    {"role": "user1", "content": "æˆ‘è®¤ä¸ºæˆ‘ä»¬åº”è¯¥...", "expected_intervention": False},
                    {"role": "user1", "content": "è€Œä¸”è¿˜æœ‰ä¸€ç‚¹...", "expected_intervention": False},
                    {"role": "user1", "content": "å¦å¤–...", "expected_intervention": False},
                    {"role": "user2", "content": "ç­‰ç­‰ï¼Œè®©æˆ‘è¯´è¯...", "expected_intervention": True},
                    {"role": "user1", "content": "è¿˜æœ‰ä¸€ä¸ªé‡è¦çš„ç‚¹...", "expected_intervention": True},
                ],
                expected_interventions=[4, 5],
                expected_timing=[1.5, 1.0],
                expected_strategy="compromising"
            ),
            
            # åœºæ™¯4: æƒ…ç»ªåŒ–è¡¨è¾¾
            TestScenario(
                name="æƒ…ç»ªåŒ–è¡¨è¾¾",
                description="æ£€æµ‹å¾®å¦™çš„æƒ…ç»ªä¿¡å·",
                messages=[
                    {"role": "user1", "content": "è¿™ä¸ªæƒ³æ³•å¤ªè’è°¬äº†", "expected_intervention": True},
                    {"role": "user2", "content": "ä½ å‡­ä»€ä¹ˆè¿™æ ·è¯´æˆ‘", "expected_intervention": True},
                    {"role": "user1", "content": "æˆ‘å—å¤Ÿäº†ä½ çš„å€Ÿå£", "expected_intervention": True},
                ],
                expected_interventions=[1, 2, 3],
                expected_timing=[1.2, 1.0, 0.8],
                expected_strategy="collaborating"
            ),
            
            # åœºæ™¯5: æ— å†²çªåœºæ™¯ï¼ˆæ§åˆ¶ç»„ï¼‰
            TestScenario(
                name="æ— å†²çªåœºæ™¯",
                description="æ­£å¸¸è®¨è®ºï¼Œä¸åº”è¯¥è§¦å‘å¹²é¢„",
                messages=[
                    {"role": "user1", "content": "ä»Šå¤©å¤©æ°”ä¸é”™", "expected_intervention": False},
                    {"role": "user2", "content": "æ˜¯çš„ï¼Œå¾ˆé€‚åˆå‡ºå»èµ°èµ°", "expected_intervention": False},
                    {"role": "user1", "content": "æˆ‘ä»¬å¯ä»¥è®¨è®ºä¸€ä¸‹é¡¹ç›®è¿›å±•", "expected_intervention": False},
                    {"role": "user2", "content": "å¥½çš„ï¼Œæˆ‘æœ€è¿‘å®Œæˆäº†ç¬¬ä¸€éƒ¨åˆ†", "expected_intervention": False},
                ],
                expected_interventions=[],
                expected_timing=[],
                expected_strategy="none"
            )
        ]
    
    async def run_comprehensive_test(self) -> Dict:
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("ğŸ§ª å¼€å§‹ç»¼åˆæ‰“æ–­åŠŸèƒ½æµ‹è¯•")
        print("=" * 60)
        
        results = []
        
        for scenario in self.test_scenarios:
            print(f"\nğŸ“‹ æµ‹è¯•åœºæ™¯: {scenario.name}")
            print(f"æè¿°: {scenario.description}")
            
            result = await self._test_scenario(scenario)
            results.append(result)
            
            # æ‰“å°åœºæ™¯ç»“æœ
            self._print_scenario_result(result)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        overall_report = self._generate_overall_report(results)
        self._print_overall_report(overall_report)
        
        return overall_report
    
    async def _test_scenario(self, scenario: TestScenario) -> TestResult:
        """æµ‹è¯•å•ä¸ªåœºæ™¯"""
        interventions_triggered = []
        response_times = []
        
        try:
            # åˆå§‹åŒ–ç›‘æ§å™¨
            monitor = OptimizedConflictMonitor(self.api_key, self.api_base)
            await monitor.initialize()
            
            for i, message_data in enumerate(scenario.messages):
                # åˆ›å»ºæ¶ˆæ¯æ•°æ®
                message = MessageData(
                    author_id=1 if message_data["role"] == "user1" else 2,
                    author_name=message_data["role"],
                    content=message_data["content"],
                    timestamp=datetime.now(),
                    typing_duration=2.0,
                    edits_count=0,
                    reactions=[]
                )
                
                # è®°å½•å¼€å§‹æ—¶é—´
                start_time = time.time()
                
                # å¤„ç†æ¶ˆæ¯
                should_intervene, score, reason, signals = await monitor.process_message(message)
                
                # è®°å½•å“åº”æ—¶é—´
                response_time = time.time() - start_time
                response_times.append(response_time)
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¹²é¢„
                expected_intervention = message_data["expected_intervention"]
                if should_intervene:
                    interventions_triggered.append({
                        "step": i + 1,
                        "content": message_data["content"],
                        "score": score,
                        "reason": reason,
                        "response_time": response_time
                    })
                
                # éªŒè¯å¹²é¢„å†³ç­–
                if should_intervene != expected_intervention:
                    print(f"   âš ï¸ æ­¥éª¤{i+1}: å¹²é¢„å†³ç­–ä¸åŒ¹é… (å®é™…:{should_intervene}, æœŸæœ›:{expected_intervention})")
                
                print(f"  æ­¥éª¤{i+1}: {message_data['content'][:20]}... -> åˆ†æ•°:{score:.2f}, å¹²é¢„:{should_intervene}, æ—¶é—´:{response_time:.2f}s")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•åœºæ™¯å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç»“æœ
            return TestResult(
                scenario_name=scenario.name,
                interventions_triggered=[],
                response_times=[],
                accuracy=0.0,
                timing_accuracy=0.0,
                strategy_accuracy=0.0,
                overall_score=0.0
            )
        
        # è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡
        accuracy = self._calculate_accuracy(scenario, interventions_triggered)
        timing_accuracy = self._calculate_timing_accuracy(scenario, response_times)
        strategy_accuracy = self._calculate_strategy_accuracy(scenario, interventions_triggered)
        
        return TestResult(
            scenario_name=scenario.name,
            interventions_triggered=interventions_triggered,
            response_times=response_times,
            accuracy=accuracy,
            timing_accuracy=timing_accuracy,
            strategy_accuracy=strategy_accuracy,
            overall_score=(accuracy + timing_accuracy + strategy_accuracy) / 3
        )
    
    def _calculate_accuracy(self, scenario: TestScenario, interventions: List[Dict]) -> float:
        """è®¡ç®—å¹²é¢„å‡†ç¡®æ€§"""
        expected_steps = set(scenario.expected_interventions)
        actual_steps = set(intervention["step"] for intervention in interventions)
        
        # è®¡ç®—ç²¾ç¡®ç‡å’Œå¬å›ç‡
        precision = len(expected_steps & actual_steps) / len(actual_steps) if actual_steps else 1.0
        recall = len(expected_steps & actual_steps) / len(expected_steps) if expected_steps else 1.0
        
        # F1åˆ†æ•°
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        return f1_score
    
    def _calculate_timing_accuracy(self, scenario: TestScenario, response_times: List[float]) -> float:
        """è®¡ç®—å“åº”æ—¶é—´å‡†ç¡®æ€§"""
        if not scenario.expected_timing:
            return 1.0  # æ— æœŸæœ›å¹²é¢„çš„åœºæ™¯
        
        # è®¡ç®—å“åº”æ—¶é—´æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
        timing_scores = []
        for i, expected_time in enumerate(scenario.expected_timing):
            if i < len(response_times):
                actual_time = response_times[i]
                # å…è®¸30%çš„è¯¯å·®
                tolerance = expected_time * 0.3
                if abs(actual_time - expected_time) <= tolerance:
                    timing_scores.append(1.0)
                else:
                    timing_scores.append(0.0)
        
        return sum(timing_scores) / len(timing_scores) if timing_scores else 1.0
    
    def _calculate_strategy_accuracy(self, scenario: TestScenario, interventions: List[Dict]) -> float:
        """è®¡ç®—ç­–ç•¥é€‰æ‹©å‡†ç¡®æ€§"""
        if scenario.expected_strategy == "none":
            return 1.0
        
        # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºæ£€æŸ¥å®é™…é€‰æ‹©çš„TKIç­–ç•¥
        # ç›®å‰è¿”å›é»˜è®¤å€¼ï¼Œéœ€è¦æ ¹æ®å®é™…ç­–ç•¥é€‰æ‹©é€»è¾‘è°ƒæ•´
        return 0.8  # é»˜è®¤å€¼
    
    def _print_scenario_result(self, result: TestResult):
        """æ‰“å°åœºæ™¯ç»“æœ"""
        print(f"\nğŸ“Š åœºæ™¯ç»“æœ: {result.scenario_name}")
        print(f"   å‡†ç¡®æ€§: {result.accuracy:.2f}")
        print(f"   æ—¶é—´å‡†ç¡®æ€§: {result.timing_accuracy:.2f}")
        print(f"   ç­–ç•¥å‡†ç¡®æ€§: {result.strategy_accuracy:.2f}")
        print(f"   ç»¼åˆåˆ†æ•°: {result.overall_score:.2f}")
        
        if result.interventions_triggered:
            print(f"   è§¦å‘å¹²é¢„: {len(result.interventions_triggered)}æ¬¡")
            for intervention in result.interventions_triggered:
                print(f"     - æ­¥éª¤{intervention['step']}: {intervention['content'][:30]}... (åˆ†æ•°:{intervention['score']:.2f})")
    
    def _generate_overall_report(self, results: List[TestResult]) -> Dict:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        if not results:
            return {"error": "æ²¡æœ‰æµ‹è¯•ç»“æœ"}
        
        avg_accuracy = sum(r.accuracy for r in results) / len(results)
        avg_timing = sum(r.timing_accuracy for r in results) / len(results)
        avg_strategy = sum(r.strategy_accuracy for r in results) / len(results)
        avg_overall = sum(r.overall_score for r in results) / len(results)
        
        total_interventions = sum(len(r.interventions_triggered) for r in results)
        
        return {
            "total_scenarios": len(results),
            "avg_accuracy": avg_accuracy,
            "avg_timing_accuracy": avg_timing,
            "avg_strategy_accuracy": avg_strategy,
            "avg_overall_score": avg_overall,
            "total_interventions": total_interventions,
            "scenario_details": [{"name": r.scenario_name, "score": r.overall_score} for r in results]
        }
    
    def _print_overall_report(self, report: Dict):
        """æ‰“å°ç»¼åˆæŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ç»¼åˆæµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        if "error" in report:
            print(f"âŒ {report['error']}")
            return
        
        print(f"æ€»æµ‹è¯•åœºæ™¯: {report['total_scenarios']}")
        print(f"å¹³å‡å‡†ç¡®æ€§: {report['avg_accuracy']:.2f}")
        print(f"å¹³å‡æ—¶é—´å‡†ç¡®æ€§: {report['avg_timing_accuracy']:.2f}")
        print(f"å¹³å‡ç­–ç•¥å‡†ç¡®æ€§: {report['avg_strategy_accuracy']:.2f}")
        print(f"å¹³å‡ç»¼åˆåˆ†æ•°: {report['avg_overall_score']:.2f}")
        print(f"æ€»å¹²é¢„æ¬¡æ•°: {report['total_interventions']}")
        
        print("\nåœºæ™¯è¯¦æƒ…:")
        for detail in report['scenario_details']:
            status = "âœ…" if detail['score'] >= 0.7 else "âš ï¸" if detail['score'] >= 0.5 else "âŒ"
            print(f"  {status} {detail['name']}: {detail['score']:.2f}")
    
    async def test_real_time_performance(self) -> Dict:
        """æµ‹è¯•å®æ—¶æ€§èƒ½"""
        print("\nâš¡ å®æ—¶æ€§èƒ½æµ‹è¯•")
        print("-" * 40)
        
        try:
            monitor = OptimizedConflictMonitor(self.api_key, self.api_base)
            await monitor.initialize()
            
            # æ¨¡æ‹Ÿé«˜å¹¶å‘æ¶ˆæ¯
            concurrent_messages = [
                "è¿™ä¸ªæƒ³æ³•å¤ªè’è°¬äº†ï¼",
                "ä½ æ€»æ˜¯è¿™æ ·ä¸è´Ÿè´£ä»»",
                "æˆ‘å—å¤Ÿäº†ä½ çš„å€Ÿå£",
                "ä½ é”™äº†ï¼Œè¿™æ ·ç»å¯¹ä¸è¡Œ",
                "ä½ ä»ä¸è€ƒè™‘åˆ«äººçš„æƒ³æ³•"
            ]
            
            start_time = time.time()
            
            # å¹¶å‘å¤„ç†
            tasks = []
            for i, content in enumerate(concurrent_messages):
                message = MessageData(
                    author_id=i % 2 + 1,
                    author_name=f"user{i % 2 + 1}",
                    content=content,
                    timestamp=datetime.now(),
                    typing_duration=1.0,
                    edits_count=0,
                    reactions=[]
                )
                task = monitor.process_message(message)
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            end_time = time.time()
            total_time = end_time - start_time
            
            # åˆ†æç»“æœ
            successful_results = [r for r in results if not isinstance(r, Exception)]
            intervention_count = sum(1 for r in successful_results if r[0])  # should_intervene
            
            performance_metrics = {
                "total_messages": len(concurrent_messages),
                "total_processing_time": total_time,
                "avg_response_time": total_time / len(concurrent_messages),
                "intervention_rate": intervention_count / len(concurrent_messages),
                "success_rate": len(successful_results) / len(concurrent_messages)
            }
            
            print(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
            print(f"   æ€»æ¶ˆæ¯æ•°: {performance_metrics['total_messages']}")
            print(f"   æ€»å¤„ç†æ—¶é—´: {performance_metrics['total_processing_time']:.2f}s")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {performance_metrics['avg_response_time']:.3f}s")
            print(f"   å¹²é¢„ç‡: {performance_metrics['intervention_rate']:.1%}")
            print(f"   æˆåŠŸç‡: {performance_metrics['success_rate']:.1%}")
            
            return performance_metrics
            
        except Exception as e:
            print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def test_context_awareness(self) -> Dict:
        """æµ‹è¯•ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›"""
        print("\nğŸ§  ä¸Šä¸‹æ–‡æ„ŸçŸ¥æµ‹è¯•")
        print("-" * 40)
        
        try:
            monitor = OptimizedConflictMonitor(self.api_key, self.api_base)
            await monitor.initialize()
            
            # æµ‹è¯•ä¸Šä¸‹æ–‡è¿ç»­æ€§
            context_scenarios = [
                # åœºæ™¯1: æŒç»­å†²çª
                [
                    ("user1", "è¿™ä¸ªæ–¹æ¡ˆæœ‰é—®é¢˜"),
                    ("user2", "ä»€ä¹ˆé—®é¢˜ï¼Ÿ"),
                    ("user1", "è®¾è®¡ä¸åˆç†"),
                    ("user2", "ä½ æ€»æ˜¯æŒ‘æ¯›ç—…"),
                    ("user1", "ä½ é”™äº†ï¼Œè¿™æ ·ä¸è¡Œ")
                ],
                # åœºæ™¯2: å†²çªåå’Œè§£
                [
                    ("user1", "è¿™ä¸ªæƒ³æ³•å¤ªè’è°¬äº†"),
                    ("user2", "ä½ å‡­ä»€ä¹ˆè¿™æ ·è¯´æˆ‘"),
                    ("user1", "æˆ‘å—å¤Ÿäº†ä½ çš„å€Ÿå£"),
                    ("user2", "ç®—äº†ï¼Œæˆ‘ä»¬å†·é™ä¸€ä¸‹"),
                    ("user1", "ä½ è¯´å¾—å¯¹ï¼Œæˆ‘ä»¬é‡æ–°å¼€å§‹")
                ]
            ]
            
            context_results = []
            
            for scenario_idx, scenario in enumerate(context_scenarios):
                print(f"\nåœºæ™¯ {scenario_idx + 1}:")
                scenario_interventions = []
                
                for step, (author, content) in enumerate(scenario):
                    message = MessageData(
                        author_id=1 if author == "user1" else 2,
                        author_name=author,
                        content=content,
                        timestamp=datetime.now(),
                        typing_duration=2.0,
                        edits_count=0,
                        reactions=[]
                    )
                    
                    should_intervene, score, reason, signals = await monitor.process_message(message)
                    
                    if should_intervene:
                        scenario_interventions.append({
                            "step": step + 1,
                            "content": content,
                            "score": score,
                            "reason": reason
                        })
                    
                    print(f"  æ­¥éª¤{step+1}: {content} -> åˆ†æ•°:{score:.2f}, å¹²é¢„:{should_intervene}")
                
                context_results.append({
                    "scenario": scenario_idx + 1,
                    "interventions": scenario_interventions,
                    "intervention_count": len(scenario_interventions)
                })
            
            return {"context_results": context_results}
            
        except Exception as e:
            print(f"âŒ ä¸Šä¸‹æ–‡æµ‹è¯•å¤±è´¥: {e}")
            return {"error": str(e)}

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    # é…ç½®API - ä½¿ç”¨ä½ çš„å®é™…APIé…ç½®
    api_key = "sk-XGGe5y0ZvLcQVFp6XnRizs7q47gsVnAbZx0Xr2mfcVlbr99f"
    api_base = "https://api2.aigcbest.top/v1"
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ComprehensiveInterruptionTester(api_key, api_base)
    
    try:
        # è¿è¡Œç»¼åˆæµ‹è¯•
        print("ğŸš€ å¼€å§‹ç»¼åˆæ‰“æ–­åŠŸèƒ½æµ‹è¯•...")
        comprehensive_results = await tester.run_comprehensive_test()
        
        # è¿è¡Œæ€§èƒ½æµ‹è¯•
        print("\nğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•...")
        performance_results = await tester.test_real_time_performance()
        
        # è¿è¡Œä¸Šä¸‹æ–‡æµ‹è¯•
        print("\nğŸš€ å¼€å§‹ä¸Šä¸‹æ–‡æ„ŸçŸ¥æµ‹è¯•...")
        context_results = await tester.test_context_awareness()
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = {
            "comprehensive_results": comprehensive_results,
            "performance_results": performance_results,
            "context_results": context_results,
            "overall_assessment": _generate_overall_assessment(comprehensive_results, performance_results)
        }
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        return final_report
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return {"error": str(e)}

def _generate_overall_assessment(comprehensive_results, performance_results):
    """ç”Ÿæˆæ•´ä½“è¯„ä¼°"""
    if "error" in comprehensive_results or "error" in performance_results:
        return "âŒ æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆè¯„ä¼°"
    
    avg_accuracy = comprehensive_results.get("avg_accuracy", 0)
    avg_timing = comprehensive_results.get("avg_timing_accuracy", 0)
    avg_response_time = performance_results.get("avg_response_time", 999)
    
    if avg_accuracy >= 0.8 and avg_timing >= 0.8 and avg_response_time <= 2.0:
        return "âœ… ç³»ç»Ÿè¡¨ç°ä¼˜ç§€ï¼Œå¯ä»¥æŠ•å…¥ç”Ÿäº§ä½¿ç”¨"
    elif avg_accuracy >= 0.6 and avg_timing >= 0.6:
        return "âš ï¸ ç³»ç»ŸåŸºæœ¬å¯ç”¨ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–"
    else:
        return "âŒ ç³»ç»Ÿéœ€è¦é‡å¤§æ”¹è¿›"

if __name__ == "__main__":
    asyncio.run(main()) 