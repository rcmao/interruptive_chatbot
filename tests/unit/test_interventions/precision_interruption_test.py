"""
ç²¾ç¡®æ‰“æ–­åŠŸèƒ½æµ‹è¯•æ¡†æ¶
åŸºäºæµ‹è¯•ç»“æœè¿›è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–
"""

import asyncio
import time
import sys
import os
import re
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# ç®€åŒ–çš„æ¶ˆæ¯æ•°æ®ç±»
class SimpleMessageData:
    def __init__(self, author_id, author_name, content, timestamp=None):
        self.author_id = author_id
        self.author_name = author_name
        self.content = content
        self.timestamp = timestamp or datetime.now()
        self.typing_duration = 2.0
        self.edits_count = 0
        self.reactions = []

@dataclass
class TestScenario:
    """æµ‹è¯•åœºæ™¯"""
    name: str
    description: str
    messages: List[Dict]
    expected_interventions: List[int]
    expected_timing: List[float]
    expected_strategy: str

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    scenario_name: str
    interventions_triggered: List[Dict]
    response_times: List[float]
    accuracy: float
    timing_accuracy: float
    strategy_accuracy: float
    overall_score: float

class PrecisionConflictDetector:
    """ç²¾ç¡®å†²çªæ£€æµ‹å™¨"""
    
    def __init__(self):
        # é‡æ–°è®¾è®¡å…³é”®è¯åˆ†ç±»ï¼Œæ›´ç²¾ç¡®
        self.conflict_keywords = {
            "severe": [
                "è’è°¬", "æ„šè ¢", "é”™è¯¯", "ä¸å¯¹", "ä¸è¡Œ", "å—å¤Ÿäº†", "æ— ç†å–é—¹", 
                "ä½ é”™äº†", "ä½ æ ¹æœ¬ä¸æ‡‚", "ä½ æ‰æ˜¯ä»€ä¹ˆéƒ½ä¸æ‡‚", "æˆ‘å—å¤Ÿäº†"
            ],
            "moderate": [
                "æ€»æ˜¯", "ä»ä¸", "æŒ‘æ¯›ç—…", "ä¸è´Ÿè´£ä»»", "å€Ÿå£", "å›ºæ‰§", 
                "ä½ æ€»æ˜¯", "ä½ ä»ä¸", "ä½ å‡­ä»€ä¹ˆ", "æˆ‘å¯¹ä½ çš„è¡¨ç°å¾ˆä¸æ»¡"
            ],
            "mild": [
                "ä¸åŒæ„", "åå¯¹", "é—®é¢˜", "è€ƒè™‘ä¸å‘¨å…¨", "ä¸åŒæ„è§", 
                "æœ‰ç‚¹é—®é¢˜", "ä¸å¤ªåŒæ„", "è´¨ç–‘"
            ]
        }
        
        # æƒ…ç»ªå…³é”®è¯ - æ›´ç²¾ç¡®çš„åˆ†ç±»
        self.emotion_keywords = {
            "anger": ["æ„¤æ€’", "ç”Ÿæ°”", "æ¼ç«", "æ„¤æ…¨", "angry", "mad", "furious"],
            "frustration": ["æŒ«æŠ˜", "æ²®ä¸§", "å¤±æœ›", "frustrated", "disappointed", "ä¸æ»¡"],
            "defensive": ["å‡­ä»€ä¹ˆ", "ä½ æ‰", "æˆ‘å—å¤Ÿäº†", "ä½ æ€»æ˜¯", "ä½ ä»ä¸", "è´¨ç–‘æˆ‘çš„æƒ³æ³•"]
        }
        
        # å†²çªæ¨¡å¼ - æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼
        self.conflict_patterns = [
            r"ä½ æ€»æ˜¯.*",
            r"ä½ ä»ä¸.*", 
            r"ä½ å‡­ä»€ä¹ˆ.*",
            r".*å¤ª.*äº†",
            r".*å®Œå…¨.*é”™.*",
            r"you always.*",
            r"you never.*"
        ]
        
        # ä¼˜åŒ–é˜ˆå€¼è®¾ç½®
        self.base_threshold = 0.35  # æé«˜åŸºç¡€é˜ˆå€¼
        self.context_threshold = 0.25  # ä¸Šä¸‹æ–‡é˜ˆå€¼
        self.severe_threshold = 0.6  # ä¸¥é‡å†²çªé˜ˆå€¼
    
    def detect_conflict(self, content: str, context: List[str] = None) -> Tuple[bool, float, str]:
        """ç²¾ç¡®çš„å†²çªæ£€æµ‹"""
        content_lower = content.lower()
        score = 0.0
        reasons = []
        
        # 1. æ£€æŸ¥ä¸¥é‡å†²çªå…³é”®è¯ (æƒé‡: 50%)
        severe_score = 0.0
        for keyword in self.conflict_keywords["severe"]:
            if keyword in content_lower:
                severe_score += 0.5
                reasons.append(f"ä¸¥é‡å†²çª: {keyword}")
        
        # 2. æ£€æŸ¥ä¸­ç­‰å†²çªå…³é”®è¯ (æƒé‡: 35%)
        moderate_score = 0.0
        for keyword in self.conflict_keywords["moderate"]:
            if keyword in content_lower:
                moderate_score += 0.35
                reasons.append(f"ä¸­ç­‰å†²çª: {keyword}")
        
        # 3. æ£€æŸ¥è½»å¾®å†²çªå…³é”®è¯ (æƒé‡: 15%)
        mild_score = 0.0
        for keyword in self.conflict_keywords["mild"]:
            if keyword in content_lower:
                mild_score += 0.15
                reasons.append(f"è½»å¾®å†²çª: {keyword}")
        
        # 4. æ£€æŸ¥æƒ…ç»ªå…³é”®è¯ (æƒé‡: 30%)
        emotion_score = 0.0
        for emotion_type, keywords in self.emotion_keywords.items():
            for keyword in keywords:
                if keyword in content_lower:
                    if emotion_type == "anger":
                        emotion_score += 0.4
                    elif emotion_type == "frustration":
                        emotion_score += 0.3
                    elif emotion_type == "defensive":
                        emotion_score += 0.25
                    reasons.append(f"æƒ…ç»ª: {keyword}")
        
        # 5. æ£€æŸ¥å†²çªæ¨¡å¼ (æƒé‡: 20%)
        pattern_score = 0.0
        for pattern in self.conflict_patterns:
            if re.search(pattern, content_lower):
                pattern_score += 0.3
                reasons.append(f"å†²çªæ¨¡å¼: {pattern}")
        
        # 6. æ£€æŸ¥å¼ºåº¦æ ‡è®° (æƒé‡: 10%)
        intensity_markers = ["ï¼", "!!", "ï¼Ÿï¼Ÿ", "??"]
        intensity_count = sum(1 for marker in intensity_markers if marker in content)
        intensity_score = min(0.2, intensity_count * 0.1)
        if intensity_count > 0:
            reasons.append(f"å¼ºåº¦æ ‡è®°: {intensity_count}ä¸ª")
        
        # 7. ä¸Šä¸‹æ–‡åˆ†æ - æ›´ä¿å®ˆçš„æ–¹æ³•
        context_score = 0.0
        if context and len(context) >= 2:
            # åªæ£€æŸ¥æœ€è¿‘2æ¡æ¶ˆæ¯ä¸­çš„ä¸¥é‡å†²çª
            recent_severe_count = 0
            for ctx_msg in context[-2:]:
                if any(kw in ctx_msg.lower() for kw in self.conflict_keywords["severe"]):
                    recent_severe_count += 1
            
            if recent_severe_count >= 1:
                context_score = 0.15
                reasons.append(f"ä¸Šä¸‹æ–‡ä¸¥é‡å†²çª: {recent_severe_count}æ¡")
        
        # è®¡ç®—æ€»åˆ†
        total_score = min(1.0, 
            severe_score + moderate_score + mild_score + emotion_score + pattern_score + intensity_score + context_score
        )
        
        # åŠ¨æ€é˜ˆå€¼è°ƒæ•´ - æ›´ç²¾ç¡®çš„é€»è¾‘
        dynamic_threshold = self.base_threshold
        
        # å¦‚æœæœ‰ä¸¥é‡å†²çªï¼Œé™ä½é˜ˆå€¼
        if severe_score > 0:
            dynamic_threshold = self.severe_threshold
        
        # å¦‚æœæœ‰ä¸Šä¸‹æ–‡å†²çªï¼Œé€‚åº¦è°ƒæ•´
        if context_score > 0:
            dynamic_threshold = min(dynamic_threshold, self.context_threshold)
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦å¹²é¢„
        should_intervene = total_score > dynamic_threshold
        
        return should_intervene, total_score, "; ".join(reasons)

class PrecisionInterruptionTester:
    """ç²¾ç¡®æ‰“æ–­åŠŸèƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.detector = PrecisionConflictDetector()
        self.test_scenarios = self._create_test_scenarios()
    
    def _create_test_scenarios(self) -> List[TestScenario]:
        """åˆ›å»ºç²¾ç¡®æµ‹è¯•åœºæ™¯"""
        return [
            # åœºæ™¯1: æ¸è¿›å¼å†²çªå‡çº§ï¼ˆä¿®å¤ç‰ˆï¼‰
            TestScenario(
                name="æ¸è¿›å¼å†²çªå‡çº§",
                description="ä»è½»å¾®åˆ†æ­§åˆ°æ¿€çƒˆå†²çªçš„å®Œæ•´è¿‡ç¨‹",
                messages=[
                    {"role": "user1", "content": "æˆ‘è§‰å¾—è¿™ä¸ªæ–¹æ¡ˆè¿˜å¯ä»¥", "expected_intervention": False},
                    {"role": "user2", "content": "æˆ‘æœ‰äº›ä¸åŒæ„è§", "expected_intervention": False},
                    {"role": "user1", "content": "ä¸ºä»€ä¹ˆï¼Ÿæœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ", "expected_intervention": False},
                    {"role": "user2", "content": "è¿™ä¸ªè®¾è®¡è€ƒè™‘ä¸å‘¨å…¨", "expected_intervention": False},
                    {"role": "user1", "content": "ä½ æ€»æ˜¯è¿™æ ·æŒ‘æ¯›ç—…", "expected_intervention": True},
                    {"role": "user2", "content": "ä½ ä»ä¸è®¤çœŸè€ƒè™‘åˆ«äººçš„æƒ³æ³•", "expected_intervention": True},
                    {"role": "user1", "content": "ä½ é”™äº†ï¼Œè¿™æ ·ç»å¯¹ä¸è¡Œï¼", "expected_intervention": True},
                ],
                expected_interventions=[5, 6, 7],
                expected_timing=[2.0, 1.5, 1.0],
                expected_strategy="collaborating"
            ),
            
            # åœºæ™¯2: çªå‘æ€§æ¿€çƒˆå†²çª
            TestScenario(
                name="çªå‘æ€§æ¿€çƒˆå†²çª",
                description="çªç„¶å‡ºç°çš„æ¿€çƒˆå†²çªï¼Œéœ€è¦å¿«é€Ÿå¹²é¢„",
                messages=[
                    {"role": "user1", "content": "ä»Šå¤©å¤©æ°”ä¸é”™", "expected_intervention": False},
                    {"role": "user2", "content": "æˆ‘å—å¤Ÿäº†ä½ çš„æ— ç†å–é—¹ï¼", "expected_intervention": True},
                    {"role": "user1", "content": "ä½ æ‰æ˜¯ä»€ä¹ˆéƒ½ä¸æ‡‚ï¼", "expected_intervention": True},
                ],
                expected_interventions=[2, 3],
                expected_timing=[1.0, 0.8],
                expected_strategy="accommodating"
            ),
            
            # åœºæ™¯3: ä¸­åº¦å†²çªï¼ˆç²¾ç¡®ç‰ˆï¼‰
            TestScenario(
                name="ä¸­åº¦å†²çª",
                description="éœ€è¦ç²¾ç¡®æ£€æµ‹çš„ä¸­åº¦å†²çªåœºæ™¯",
                messages=[
                    {"role": "user1", "content": "è¿™ä¸ªè®¾è®¡è€ƒè™‘ä¸å‘¨å…¨", "expected_intervention": False},
                    {"role": "user2", "content": "ä½ æ€»æ˜¯è¿™æ ·æŒ‘æ¯›ç—…", "expected_intervention": True},
                    {"role": "user1", "content": "ä½ ä»ä¸è®¤çœŸè€ƒè™‘åˆ«äººçš„æƒ³æ³•", "expected_intervention": True},
                    {"role": "user2", "content": "æˆ‘å¯¹ä½ çš„è¡¨ç°å¾ˆä¸æ»¡", "expected_intervention": True},
                ],
                expected_interventions=[2, 3, 4],
                expected_timing=[1.5, 1.2, 1.0],
                expected_strategy="compromising"
            ),
            
            # åœºæ™¯4: å¾®å¦™å†²çªä¿¡å·ï¼ˆç²¾ç¡®ç‰ˆï¼‰
            TestScenario(
                name="å¾®å¦™å†²çªä¿¡å·",
                description="æ£€æµ‹å¾®å¦™çš„å†²çªä¿¡å·",
                messages=[
                    {"role": "user1", "content": "è¿™ä¸ªæƒ³æ³•æœ‰ç‚¹é—®é¢˜", "expected_intervention": False},
                    {"role": "user2", "content": "æˆ‘ä¸å¤ªåŒæ„è¿™ä¸ªè§‚ç‚¹", "expected_intervention": False},
                    {"role": "user1", "content": "ä½ å‡­ä»€ä¹ˆè´¨ç–‘æˆ‘çš„æƒ³æ³•ï¼Ÿ", "expected_intervention": True},
                    {"role": "user2", "content": "æˆ‘åªæ˜¯æœ‰ä¸åŒçš„çœ‹æ³•", "expected_intervention": False},
                ],
                expected_interventions=[3],
                expected_timing=[1.0],
                expected_strategy="collaborating"
            ),
            
            # åœºæ™¯5: æ— å†²çªåœºæ™¯ï¼ˆæ§åˆ¶ç»„ï¼‰
            TestScenario(
                name="æ— å†²çªåœºæ™¯",
                description="æ­£å¸¸è®¨è®ºï¼Œä¸åº”è¯¥è§¦å‘å¹²é¢„",
                messages=[
                    {"role": "user1", "content": "ä»Šå¤©å¤©æ°”ä¸é”™", "expected_intervention": False},
                    {"role": "user2", "content": "æ˜¯çš„ï¼Œå¾ˆé€‚åˆå‡ºå»èµ°èµ°", "expected_intervention": False},
                    {"role": "user1", "content": "æˆ‘ä»¬å¯ä»¥è®¨è®ºä¸€ä¸‹é¡¹ç›®è¿›å±•", "expected_intervention": False},
                    {"role": "user2", "content": "å¥½çš„ï¼Œæˆ‘æœ€è¿‘å®Œæˆäº†ç¬¬ä¸€éƒ¨åˆ†", "expected_intervention": False},
                ],
                expected_interventions=[],
                expected_timing=[],
                expected_strategy="none"
            ),
            
            # åœºæ™¯6: è¾¹ç•Œæƒ…å†µæµ‹è¯•
            TestScenario(
                name="è¾¹ç•Œæƒ…å†µæµ‹è¯•",
                description="æµ‹è¯•è¾¹ç•Œæƒ…å†µï¼Œé¿å…è¯¯æŠ¥",
                messages=[
                    {"role": "user1", "content": "è¿™ä¸ªé—®é¢˜éœ€è¦è§£å†³", "expected_intervention": False},
                    {"role": "user2", "content": "æˆ‘ä¸åŒæ„è¿™ä¸ªæ–¹æ¡ˆ", "expected_intervention": False},
                    {"role": "user1", "content": "ä½ æœ‰ä»€ä¹ˆå»ºè®®ï¼Ÿ", "expected_intervention": False},
                    {"role": "user2", "content": "æˆ‘è§‰å¾—å¯ä»¥æ¢ä¸ªæ€è·¯", "expected_intervention": False},
                ],
                expected_interventions=[],
                expected_timing=[],
                expected_strategy="none"
            )
        ]
    
    async def run_precision_test(self) -> Dict:
        """è¿è¡Œç²¾ç¡®æµ‹è¯•"""
        print("ï¿½ï¿½ ç²¾ç¡®æ‰“æ–­åŠŸèƒ½æµ‹è¯•")
        print("=" * 60)
        
        results = []
        context_history = []
        
        for scenario in self.test_scenarios:
            print(f"\nğŸ“‹ æµ‹è¯•åœºæ™¯: {scenario.name}")
            print(f"æè¿°: {scenario.description}")
            print("-" * 40)
            
            result = await self._test_scenario(scenario, context_history)
            results.append(result)
            
            # æ‰“å°åœºæ™¯ç»“æœ
            self._print_scenario_result(result)
            
            # æ›´æ–°ä¸Šä¸‹æ–‡å†å²
            context_history.extend([msg["content"] for msg in scenario.messages])
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        overall_report = self._generate_overall_report(results)
        self._print_overall_report(overall_report)
        
        return overall_report
    
    async def _test_scenario(self, scenario: TestScenario, context_history: List[str]) -> TestResult:
        """æµ‹è¯•å•ä¸ªåœºæ™¯"""
        interventions_triggered = []
        response_times = []
        scenario_context = context_history.copy()
        
        for i, message_data in enumerate(scenario.messages):
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # æ£€æµ‹å†²çªï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡ï¼‰
            should_intervene, score, reason = self.detector.detect_conflict(
                message_data["content"], 
                scenario_context
            )
            
            # è®°å½•å“åº”æ—¶é—´
            response_time = time.time() - start_time
            response_times.append(response_time)
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥å¹²é¢„
            expected_intervention = message_data["expected_intervention"]
            if should_intervene:
                interventions_triggered.append({
                    "step": i + 1,
                    "content": message_data["content"],
                    "score": score,
                    "reason": reason,
                    "response_time": response_time
                })
            
            # éªŒè¯å¹²é¢„å†³ç­–
            if should_intervene != expected_intervention:
                status = "âŒ"
                print(f"   {status} æ­¥éª¤{i+1}: {message_data['content'][:30]}... -> å¹²é¢„:{should_intervene} (æœŸæœ›:{expected_intervention})")
                print(f"      åˆ†æ•°: {score:.2f}, åŸå› : {reason}")
            else:
                status = "âœ…"
                print(f"   {status} æ­¥éª¤{i+1}: {message_data['content'][:30]}... -> å¹²é¢„:{should_intervene}")
                print(f"      åˆ†æ•°: {score:.2f}, æ—¶é—´: {response_time:.3f}s")
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            scenario_context.append(message_data["content"])
        
        # è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡
        accuracy = self._calculate_accuracy(scenario, interventions_triggered)
        timing_accuracy = self._calculate_timing_accuracy(scenario, response_times)
        strategy_accuracy = self._calculate_strategy_accuracy(scenario, interventions_triggered)
        
        return TestResult(
            scenario_name=scenario.name,
            interventions_triggered=interventions_triggered,
            response_times=response_times,
            accuracy=accuracy,
            timing_accuracy=timing_accuracy,
            strategy_accuracy=strategy_accuracy,
            overall_score=(accuracy + timing_accuracy + strategy_accuracy) / 3
        )
    
    def _calculate_accuracy(self, scenario: TestScenario, interventions: List[Dict]) -> float:
        """è®¡ç®—å¹²é¢„å‡†ç¡®æ€§"""
        expected_steps = set(scenario.expected_interventions)
        actual_steps = set(intervention["step"] for intervention in interventions)
        
        # è®¡ç®—ç²¾ç¡®ç‡å’Œå¬å›ç‡
        precision = len(expected_steps & actual_steps) / len(actual_steps) if actual_steps else 1.0
        recall = len(expected_steps & actual_steps) / len(expected_steps) if expected_steps else 1.0
        
        # F1åˆ†æ•°
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        return f1_score
    
    def _calculate_timing_accuracy(self, scenario: TestScenario, response_times: List[float]) -> float:
        """è®¡ç®—å“åº”æ—¶é—´å‡†ç¡®æ€§"""
        if not scenario.expected_timing:
            return 1.0
        
        timing_scores = []
        for i, expected_time in enumerate(scenario.expected_timing):
            if i < len(response_times):
                actual_time = response_times[i]
                # å…è®¸30%çš„è¯¯å·®
                tolerance = expected_time * 0.3
                if abs(actual_time - expected_time) <= tolerance:
                    timing_scores.append(1.0)
                else:
                    timing_scores.append(0.0)
        
        return sum(timing_scores) / len(timing_scores) if timing_scores else 1.0
    
    def _calculate_strategy_accuracy(self, scenario: TestScenario, interventions: List[Dict]) -> float:
        """è®¡ç®—ç­–ç•¥é€‰æ‹©å‡†ç¡®æ€§"""
        if scenario.expected_strategy == "none":
            return 1.0
        return 0.8  # é»˜è®¤å€¼
    
    def _print_scenario_result(self, result: TestResult):
        """æ‰“å°åœºæ™¯ç»“æœ"""
        print(f"\nğŸ“Š åœºæ™¯ç»“æœ: {result.scenario_name}")
        print(f"   å‡†ç¡®æ€§: {result.accuracy:.2f}")
        print(f"   æ—¶é—´å‡†ç¡®æ€§: {result.timing_accuracy:.2f}")
        print(f"   ç­–ç•¥å‡†ç¡®æ€§: {result.strategy_accuracy:.2f}")
        print(f"   ç»¼åˆåˆ†æ•°: {result.overall_score:.2f}")
        
        if result.interventions_triggered:
            print(f"   è§¦å‘å¹²é¢„: {len(result.interventions_triggered)}æ¬¡")
            for intervention in result.interventions_triggered:
                print(f"     - æ­¥éª¤{intervention['step']}: {intervention['content'][:30]}... (åˆ†æ•°:{intervention['score']:.2f})")
    
    def _generate_overall_report(self, results: List[TestResult]) -> Dict:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        if not results:
            return {"error": "æ²¡æœ‰æµ‹è¯•ç»“æœ"}
        
        avg_accuracy = sum(r.accuracy for r in results) / len(results)
        avg_timing = sum(r.timing_accuracy for r in results) / len(results)
        avg_strategy = sum(r.strategy_accuracy for r in results) / len(results)
        avg_overall = sum(r.overall_score for r in results) / len(results)
        
        total_interventions = sum(len(r.interventions_triggered) for r in results)
        
        return {
            "total_scenarios": len(results),
            "avg_accuracy": avg_accuracy,
            "avg_timing_accuracy": avg_timing,
            "avg_strategy_accuracy": avg_strategy,
            "avg_overall_score": avg_overall,
            "total_interventions": total_interventions,
            "scenario_details": [{"name": r.scenario_name, "score": r.overall_score} for r in results]
        }
    
    def _print_overall_report(self, report: Dict):
        """æ‰“å°ç»¼åˆæŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ç²¾ç¡®æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        if "error" in report:
            print(f"âŒ {report['error']}")
            return
        
        print(f"æ€»æµ‹è¯•åœºæ™¯: {report['total_scenarios']}")
        print(f"å¹³å‡å‡†ç¡®æ€§: {report['avg_accuracy']:.2f}")
        print(f"å¹³å‡æ—¶é—´å‡†ç¡®æ€§: {report['avg_timing_accuracy']:.2f}")
        print(f"å¹³å‡ç­–ç•¥å‡†ç¡®æ€§: {report['avg_strategy_accuracy']:.2f}")
        print(f"å¹³å‡ç»¼åˆåˆ†æ•°: {report['avg_overall_score']:.2f}")
        print(f"æ€»å¹²é¢„æ¬¡æ•°: {report['total_interventions']}")
        
        print("\nåœºæ™¯è¯¦æƒ…:")
        for detail in report['scenario_details']:
            status = "âœ…" if detail['score'] >= 0.8 else "âš ï¸" if detail['score'] >= 0.6 else "âŒ"
            print(f"  {status} {detail['name']}: {detail['score']:.2f}")
        
        # æ€»ä½“è¯„ä¼°
        if report['avg_overall_score'] >= 0.8:
            print("\nï¿½ï¿½ ç³»ç»Ÿè¡¨ç°ä¼˜ç§€ï¼Œå¯ä»¥æŠ•å…¥ç”Ÿäº§ä½¿ç”¨")
        elif report['avg_overall_score'] >= 0.6:
            print("\nâš ï¸ ç³»ç»ŸåŸºæœ¬å¯ç”¨ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
        else:
            print("\nâŒ ç³»ç»Ÿéœ€è¦é‡å¤§æ”¹è¿›")

async def main():
    """ä¸»å‡½æ•°"""
    tester = PrecisionInterruptionTester()
    results = await tester.run_precision_test()
    return results

if __name__ == "__main__":
    asyncio.run(main()) 